---
title: "Linear_regression"
author: "Matthew Chen, Jasper Tsai, Mark Faynboym"
date: '2022-12-04'
output: html_document
---
```{r}
library(MASS)
library(leaps)
```

```{r}
# import data 
df = read.csv('data/data_clean.csv')
# remove location column
df = df[-4]

df
```
```{r}
# change categorical variables to R Factors
df$size = as.factor(df$size)
df$python = as.factor(df$python)
df$visual_software = as.factor(df$visual_software)
df$ML_software = as.factor(df$ML_software)
df$spark = as.factor(df$spark)
df$aws = as.factor(df$aws)
df$excel = as.factor(df$excel)
df$sql = as.factor(df$sql)
df$sas = as.factor(df$sas)
df$hadoop = as.factor(df$hadoop)
df$degree = as.factor(df$degree)
df$in_CA = as.factor(df$in_CA)
df$senior_status = as.factor(df$senior_status)
```

## Model 1: Linear model with all first order terms
Here we have p=22 n=433 observations. We have R2=0.3865 and R2a=0.3552.
```{r}
model1 = lm(avg_salary_k~., data=df)
summary(model1)
```
```{r}
# Model diagnostics
par(mfrow=c(2, 2))
plot(model1)
```
In the model diagnostics, we find that residuals have no pattern, are centered about zero,
and have no sign of heteroskedasticity. In the normal-QQ plot, we find that there
are heavy right tails, though this may be caused by outliers in Y (we will examine these later).
Overall, this model is reasonable.

## Exploration of transformation in Y
Since we have heavy tails in the residual distribution, we conduct the box-cox procedure
to see if a transformation of the response variable would be beneficial.
```{r}
boxcox(model1, data=df)
```
The Box-Cox suggests a square root transformation of the data

## Refit Model 1 using a square root transform in Y
```{r}
df_sqrtY = df
df_sqrtY$avg_salary_k = sqrt(df$avg_salary_k)
```

```{r}
model1_sqrt = lm(avg_salary_k~., data=df_sqrtY)
summary(model1_sqrt)
```
```{r}
# model diagnostics
par(mfrow=c(2, 2))
plot(model1_sqrt)
```
In the QQ plot we see that the distribution is much less heavy tailed than before, and
R2 and R2a also remain mostly unchanged. Thus, these heavy tails may be caused by
a handful of outliers in Y. We continue using the square root transformation of our response variable.

## Model 2: inclusion of all two-way interaction terms
Since R2 is low in Model 1, we attempt to fit a second order model with all interaction terms
in an attempt to gain a better fit of the data.


```{r}
# Second order interaction model
model2 = lm(avg_salary_k~(.)^2, data=df)
summary(model2)
```
```{r}
boxcox(model2, data=df)
```
```{r}
# Second order interaction model
model2_sqrtY = lm(avg_salary_k~(.)^2, data=df_sqrtY)
summary(model2_sqrtY)
```

Here we have p=211 regression coefficients. While the multiple R2 has increased significantly, the adjusted R2 has not 
changed by much (due to the drastic decrease in degrees of freedom). However, we realize that most of the terms are not significant so we attempt to fit a smaller subset of model2 using stepwise regression.


```{r}
fit0 = lm(avg_salary_k~1, data=df_sqrtY)
step.aic = stepAIC(fit0, scope=list(upper=model2_sqrtY, lower=fit0), trace=FALSE)
step.aic$anova
```

```{r}
# fit selected model
model2_aic = lm(avg_salary_k ~ senior_status + in_CA + python + degree + visual_software + 
    age + rating + sas + aws + ML_software + hadoop + sql + python:aws + 
    in_CA:ML_software + age:ML_software + age:rating + python:rating + 
    visual_software:hadoop + python:hadoop + sas:sql, data=df_sqrtY)
summary(model2_aic)
```
```{r}
# model diagnostics
par(mfrow=c(2, 2))
plot(model2_aic)
```

While adjusted R2 here is not much lower than the multiple R2, the difference compared to the first
order model is minimal. The model diagnostics also do not improve much over
Model 1. Especially since interaction terms are harder to interpret, we
move forward with first order models. 

## Subset of Model 1
We also look into the possibility of subsetting Model1.

```{r}
fit0 = lm(avg_salary_k~1, data=df_sqrtY)
step.aic = stepAIC(fit0, scope=list(upper=model1_sqrt, lower=fit0), trace=FALSE)
step.aic$anova
```

```{r}
# fit the selected model
model1_aic = lm(avg_salary_k ~ senior_status + in_CA + python + degree + visual_software + 
    age + rating + sas + aws + ML_software + excel + hadoop, data=df_sqrtY)
summary(model1_aic)
```
Since adjusted R2 barely changed (-0.006), we choose this smaller model as our
final model based on the principle of parsimony. 

```{r}
# model diagnostics
par(mfrow=c(2, 2))
plot(model1_aic)
```
Model diagnostics remains similar compared to model 1. 

## Analysis of Outliers
In the residual plots above, we notice points that are potentially outlying in Y
and are creating heavy tails in the residual distribution...

```{r}
#Bonferonni outlier test
library(car)
outlierTest(model1_aic,cutoff=0.05,nmax=15)
```

```{r}
#Cook's distance
library(base)
Cookvalue=4/(length(df$avg_salary_k*summary(model2_aic)$df[2])) #Cook's distance threshold model 2
Cookvalue
cooks.distance(model2_aic)[267] #Cook's distance for our outlier from the Bonferonni test
```

```{r}
# check for points outlying in X
leverage = influence(model2_aic)$hat # leverage values
list=leverage[leverage > 2*length(model2_aic$coefficients)/length(model2_aic$residuals)] #outliers in X
list
Cooks_distances=cooks.distance(model2_aic)[attributes(list)$names] #none of the outliers in X are influential
High_influence_vars=Cooks_distances[Cooks_distances>Cookvalue]
High_influence_vars
```

We try to refit model1_aic after removing the outlier
```{r}
df_out = df_sqrtY[-c(as.numeric(attributes(High_influence_vars)$names)), ]
head(df_out)
```
####
## Model 1: Linear model with all first order terms
Here we have p=22 n=432 observations. We have R2=0.3865 and R2a=0.3552.
```{r}
model1N = lm(avg_salary_k~., data=df_out)
summary(model1N)

# Model diagnostics
par(mfrow=c(2, 2))
plot(model1N)
```
In the model diagnostics, we find that residuals have no pattern, are centered about zero,
and have no sign of heteroskedasticity. In the normal-QQ plot, we find that there
are heavy right tails, though this may be caused by outliers in Y (we will examine these later).
Overall, this model is reasonable.


```{r}
model1_sqrtN = lm(avg_salary_k~., data=df_out)
summary(model1_sqrtN)
```
```{r}
# model diagnostics
par(mfrow=c(2, 2))
plot(model1_sqrtN)
```
In the QQ plot we see that the distribution is much less heavy tailed than before, and
R2 and R2a also remain mostly unchanged. Thus, these heavy tails may be caused by
a handful of outliers in Y. For now, we continue exploring fitting models using
the original Y.

## Model 2: inclusion of all two-way interaction terms
Since R2 is low in Model 1, we attempt to fit a second order model with all interaction terms
in an attempt to gain a better fit of the data.
```{r}
# Second order interaction model
model2N = lm(avg_salary_k~(.)^2, data=df_out)
summary(model2N)
```
Here we have p=211 regression coefficients. While the multiple R2 has increased significantly, the adjusted R2 has not 
changed by much (due to the drastic decrease in degrees of freedom). However, we realize that most of the terms are not significant so we attempt to fit a smaller subset of model2 using stepwise regression.

```{r}
fit0N = lm(avg_salary_k~1, data=df_out)
step.aicN = stepAIC(fit0N, scope=list(upper=model2N, lower=fit0N), trace=FALSE)
step.aicN$anova
```

```{r}
# fit selected model
model2_aicN = lm(avg_salary_k ~ senior_status + in_CA + python + visual_software + 
    degree + rating + sas + age + ML_software + aws + hadoop + 
    sql + in_CA:ML_software + age:ML_software + python:aws + 
    rating:hadoop + visual_software:hadoop + python:hadoop + 
    sas:ML_software, data=df_out)
summary(model2_aicN)
```
```{r}
# model diagnostics
par(mfrow=c(2, 2))
plot(model2_aicN)
```




