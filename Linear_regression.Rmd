---
title: "Linear_regression"
author: "Matthew Chen, Jasper Tsai, Mark Faynboym"
date: '2022-12-04'
output: 
  html_document:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


```{r}
library(MASS)
library(leaps)
library(caret)
```


```{r}
# import data 
df = read.csv('data/data_clean.csv')
# remove 'job_location' column
df = df[-4]

head(df)
```



```{r}
# change categorical variables to R Factors
df$size = as.factor(df$size)
df$python = as.factor(df$python)
df$visual_software = as.factor(df$visual_software)
df$ML_software = as.factor(df$ML_software)
df$spark = as.factor(df$spark)
df$aws = as.factor(df$aws)
df$excel = as.factor(df$excel)
df$sql = as.factor(df$sql)
df$sas = as.factor(df$sas)
df$hadoop = as.factor(df$hadoop)
df$degree = as.factor(df$degree)
df$in_CA = as.factor(df$in_CA)
df$senior_status = as.factor(df$senior_status)
```

# No interaction
## model1: Linear model with all first order terms
Here we have p=22 n=433 observations. We have R2=0.3865 and R2a=0.3552.
```{r}
#Fit First order model with full data
model1 = lm(avg_salary_k~., data=df)
summary(model1)
```



```{r}
# Model diagnostics
par(mfrow=c(2, 2))
plot(model1)
```


In the model diagnostics, we find that residuals have no pattern, are centered about zero,
and have no sign of heteroskedasticity. In the normal-QQ plot, we find that there
are heavy right tails, though this may be caused by outliers in Y (we will examine these later).
Overall, this model is reasonable.

### Exploration of transformation in Y with original full data with model1
Since we have heavy tails in the residual distribution, we conduct the box-cox procedure
to see if a transformation of the response variable would be beneficial.
```{r}
boxcox(model1, data=df)
```

The Box-Cox suggests a square root transformation of the data\
**we will continue to use SQRT(Y) data for rest of this section**

## model1_sqrt: Refit model1 using a square root transform in Y
```{r}
#create new dataset called df_sqrtY with sqrt(Y) as avg_salary_k
df_sqrtY = df
df_sqrtY$avg_salary_k = sqrt(df$avg_salary_k)
```

Here we have p=22 n=433 observations. We have R2=0.3887 and R2a=0.3574.
```{r}
model1_sqrt = lm(avg_salary_k~., data=df_sqrtY)
summary(model1_sqrt)
```

```{r}
# model diagnostics
par(mfrow=c(2, 2))
plot(model1_sqrt)
```
In the QQ plot we see that the distribution is much less heavy tailed than before, and
R2 and R2a also remain mostly unchanged. Thus, these tails may be caused by
a handful of outliers in Y. We continue using the square root transformation of our response variable.

## mordel1_sqrt_aic: AIC of model1_sqrt
We want to use AIC to see if we can get a better model

```{r}
fit0 = lm(avg_salary_k~1, data=df_sqrtY)
step.aic = stepAIC(fit0, scope=list(upper=model1_sqrt, lower=fit0), trace=FALSE)
step.aic$anova
```

### fit model1_sqrt_aic from aic criterion
We have R2=0.3731 and R2a=0.3537.

```{r}
# fit the selected model
model1_sqrt_aic = lm(avg_salary_k ~ senior_status + in_CA + python + degree + visual_software + 
    age + rating + sas + aws + ML_software + excel + hadoop, data=df_sqrtY)
summary(model1_sqrt_aic)
```

```{r}
# model diagnostics
par(mfrow=c(2, 2))
plot(model1_sqrt_aic)
```

The model is valid.



# Interaction

## model2: inclusion of all two-way interaction terms
Since R2 is low in Model 1, we attempt to fit a second order model with all interaction terms
in an attempt to gain a better fit of the data.

p=211, n = 422. We have R2 = 0.6679, R2a = 0.3538
```{r}
# Second order interaction model
model2 = lm(avg_salary_k~(.)^2, data=df)
summary(model2)
```

```{r}
# model diagnostics
par(mfrow=c(2, 2))
plot(model2)
```

In the model diagnostics, we find that residuals have no pattern, are centered about zero,
and have no sign of heteroskedasticity. In the normal-QQ plot, we find that there
are some right tails, though this may be caused by outliers in Y (we will examine these later).
Overall, this model is reasonable.

### Exploration of transformation in Y with original full data with model2
Since we have tails in the residual distribution, we conduct the box-cox procedure
to see if a transformation of the response variable would be beneficial.

```{r}
boxcox(model2, data=df)
```

The Box-Cox suggests a square root transformation of the data
**we will continue to use SQRT(Y) data for rest of this section**

## model2_sqrt: Refit model2 using a square root transform in Y
We have R2 = 0.6571, R2a = 0.3328
```{r}
# Second order interaction model
model2_sqrt = lm(avg_salary_k~(.)^2, data=df_sqrtY)
summary(model2_sqrt)
```

```{r}
# model diagnostics
par(mfrow=c(2, 2))
plot(model2_sqrt)
```

Here we have p=211 regression coefficients. While the multiple R2 has increased significantly, the adjusted R2 has not 
changed by much (due to the drastic decrease in degrees of freedom). However, we realize that most of the terms are not significant so we attempt to fit a smaller subset of model2_sqrt using stepwise regression.




## model2_sqrt_aic: AIC of model2_sqrt

```{r}
fit0 = lm(avg_salary_k~1, data=df_sqrtY)
step.aic = stepAIC(fit0, scope=list(upper=model2_sqrt, lower=fit0), trace=FALSE)
step.aic$anova
```

We have R2 = 0.4092, R2a = 0.3791
```{r}
# fit selected model
model2_sqrt_aic = lm(avg_salary_k ~ senior_status + in_CA + python + degree + visual_software + 
    age + rating + sas + aws + ML_software + hadoop + sql + python:aws + 
    in_CA:ML_software + age:ML_software + age:rating + python:rating + 
    visual_software:hadoop + python:hadoop + sas:sql, data=df_sqrtY)
summary(model2_sqrt_aic)
```

```{r}
# model diagnostics
par(mfrow=c(2, 2))
plot(model2_sqrt_aic)
```

The model is valid.



# Analysis of Outliers
**We will continue to do the rest of the outlier analysis with model2_sqrt_aic**
In the residual plots above, we notice points that are potentially outlying in Y
and are creating heavy tails in the residual distribution...

## Bonferroni Outlier test (Outliers in Y)
```{r}
#Bonferonni outlier test
require(car)
# return 
outlierTest(model2_sqrt_aic,cutoff=0.05,nmax=15)
```

```{r}
#Cook's distance
library(base)
Cookvalue=4/(length(df_sqrtY$avg_salary_k*summary(model2_sqrt_aic)$df[2])) #Cook's distance threshold model 2
paste("Cook's distance value is :", Cookvalue)
```

```{r}
cooks.distance(model2_sqrt_aic)[204] #Cook's distance for our outlier from the Bonferonni test
```

There is NO influential cases for outlying in Y


## Leverage (Outliers in X)
```{r}
# check for points outlying in X
leverage = influence(model2_sqrt_aic)$hat # leverage values
list=leverage[leverage > 2*length(model2_sqrt_aic$coefficients)/length(model2_sqrt_aic$residuals)] #outliers in X
list
```

```{r}
#Cook's distance
Cooks_distances=cooks.distance(model2_sqrt_aic)[attributes(list)$names] #
High_influence_vars=Cooks_distances[Cooks_distances>Cookvalue]
High_influence_vars
```

High influential cases from outlying in X are identified.



## Remove outliers identified that are influential
We try to refit model2_sqrt_aic after removing the outlier\
new dataframe called **df_out** that uses df_sqrtY and remove outliers from that dataframe
```{r}
# remove outliers from df_sqrtY and create new df_out
df_out = df_sqrtY[-c(as.numeric(attributes(High_influence_vars)$names)), ]
head(df_out)
```

## model2_sqrt_aic_out: Directly refit model2_sqrt_aic onto data without outliers
p = 22, n = 424, R2 = 0.4117, R2a = 0.381
```{r}
model2_sqrt_aic_out = lm(avg_salary_k ~ senior_status + in_CA + python + degree + visual_software + 
    age + rating + sas + aws + ML_software + hadoop + sql + python:aws + 
    in_CA:ML_software + age:ML_software + age:rating + python:rating + 
    visual_software:hadoop + python:hadoop + sas:sql, data = df_out)
summary = summary(model2_sqrt_aic_out)
summary
```

### Anova of model2_sqrt_aic_out
```{r}
anova(model2_sqrt_aic_out)
```

### Model diagnostics
```{r}
par(mfrow=c(2, 2))
plot(model2_sqrt_aic_out)
```


## Confidence Interval for model2_sqrt_aic_out
```{r}
#Confidence Interval
Estimate = summary$coefficients[,1]
n=length(df_out$avg_salary_k)
s=summary$coefficients[,2]
margin = s*qt(0.975,df=n-2)
Lower_Bound = Estimate-s
Upper_Bound =Estimate+s
Confidence_Intervals=round(cbind(Lower_Bound,Estimate,Upper_Bound),3)
Confidence_Intervals
```

### Confidence Intervals of Coefficients for model2_sqrt_aic_out
```{r}
x=1:length(Estimate)
data=data.frame(x,Estimate)

require(ggplot2)
ggplot(data,aes(x=x, y=Estimate)) + geom_point() + geom_errorbar(aes(ymin = Lower_Bound, ymax = Upper_Bound)) + labs(title="Confidence Intervals of Coefficients",
       x="Beta Coefficient", 
       y="Value")
```


## Calculate Pressp for final selected model, model2_sqrt_aci_out
```{r}
# Press P
LOOCV = train(avg_salary_k ~ senior_status + in_CA + python + degree + visual_software + 
    age + rating + sas + aws + ML_software + hadoop + sql + python:aws + 
    in_CA:ML_software + age:ML_software + age:rating + python:rating + 
    visual_software:hadoop + python:hadoop + sas:sql, data = df_out, method = "lm", trControl = trainControl(method = "LOOCV"))
MSE_LOOCV=as.numeric(LOOCV$results[2]^2)
PressP=MSE_LOOCV*n
PressP
```

```{r}
SSE=summary(model2_sqrt_aic_out)$sigma^2*n
SSE
```





